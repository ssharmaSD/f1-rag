"""
Test script for ethical web scraping functionality
"""
import sys
from pathlib import Path

# Add utils to path
sys.path.append(str(Path(__file__).parent))

def test_ethical_scraping():
    """Test the ethical web scraping system"""
    print("ğŸŒ Testing Ethical F1 Web Scraping System")
    print("=" * 50)
    
    try:
        from utils.ethical_web_scraper import EthicalF1WebScraper
        
        # Initialize ethical scraper
        print("Initializing ethical web scraper...")
        scraper = EthicalF1WebScraper()
        print("âœ… Ethical web scraper initialized")
        
        # Show ethical sources info
        print("\nğŸ“‹ Ethical Sources Information:")
        sources_info = scraper.get_ethical_sources_info()
        
        print("\nğŸ”„ RSS Sources:")
        for source_id, source in sources_info['rss_sources'].items():
            print(f"  â€¢ {source['name']}: {source['reason']}")
        
        print("\nğŸ”Œ API Sources:")
        for source_id, source in sources_info['api_sources'].items():
            print(f"  â€¢ {source['name']}: {source['reason']}")
        
        print("\nâš™ï¸ Scraping Policy:")
        policy = sources_info['scraping_policy']
        print(f"  â€¢ Respects robots.txt: {policy['respects_robots_txt']}")
        print(f"  â€¢ Rate limited: {policy['rate_limited']}")
        print(f"  â€¢ Default delay: {policy['default_delay_seconds']}s")
        print(f"  â€¢ User agent: {policy['user_agent']}")
        
        # Test RSS-only scraping (ethical and safe)
        print("\nğŸ“° Testing RSS-only scraping...")
        rss_articles = scraper.scrape_rss_feeds_only()
        print(f"âœ… Fetched {len(rss_articles)} articles from RSS feeds")
        
        if rss_articles:
            # Show sample article
            sample_article = rss_articles[0]
            print(f"\nğŸ“„ Sample RSS Article:")
            print(f"  Title: {sample_article['title']}")
            print(f"  Source: {sample_article['source']}")
            print(f"  Method: {sample_article['scraping_method']}")
            print(f"  Ethical Status: {sample_article['ethical_status']}")
            print(f"  Content length: {len(sample_article['content'])} characters")
        
        # Test API fetching (also ethical)
        print("\nğŸ”Œ Testing API data fetching...")
        api_articles = scraper.fetch_from_apis()
        print(f"âœ… Fetched {len(api_articles)} items from APIs")
        
        if api_articles:
            sample_api = api_articles[0]
            print(f"\nğŸ“Š Sample API Data:")
            print(f"  Title: {sample_api['title']}")
            print(f"  Source: {sample_api['source']}")
            print(f"  Method: {sample_api['scraping_method']}")
            print(f"  Data Type: {sample_api.get('data_type', 'N/A')}")
        
        # Test combined ethical content fetching
        print("\nğŸŒŸ Testing combined ethical content fetching...")
        all_ethical_content = scraper.fetch_all_ethical_content()
        print(f"âœ… Fetched {len(all_ethical_content)} total items ethically")
        
        # Test saving with ethical metadata
        print("\nğŸ’¾ Testing ethical content saving...")
        compliance_metadata = scraper.save_articles(all_ethical_content)
        print("âœ… Articles saved with ethical compliance metadata")
        
        print(f"\nğŸ“Š Ethical Compliance Summary:")
        compliance = compliance_metadata['ethical_compliance']
        print(f"  â€¢ Total articles: {compliance_metadata['total_articles']}")
        print(f"  â€¢ Respects robots.txt: {compliance['respects_robots_txt']}")
        print(f"  â€¢ Rate limited: {compliance['rate_limited']}")
        print(f"  â€¢ Uses RSS feeds: {compliance['uses_rss_feeds']}")
        print(f"  â€¢ Uses official APIs: {compliance['uses_official_apis']}")
        print(f"  â€¢ Methods used: {', '.join(compliance_metadata['scraping_methods'])}")
        
        print("\nğŸ‰ Ethical web scraping test completed successfully!")
        print("\nğŸ“š All content obtained through ethical means:")
        print("  âœ… RSS feeds (designed for consumption)")
        print("  âœ… Official APIs (designed for data access)")
        print("  âœ… Robots.txt compliance")
        print("  âœ… Rate limiting respected")
        print("  âœ… Educational use only")
        
    except Exception as e:
        print(f"âŒ Error during testing: {e}")
        import traceback
        traceback.print_exc()

def test_robots_txt_compliance():
    """Test robots.txt compliance checking"""
    print("\nğŸ¤– Testing Robots.txt Compliance")
    print("=" * 40)
    
    try:
        from utils.ethical_web_scraper import EthicalF1WebScraper
        
        scraper = EthicalF1WebScraper()
        
        # Test URLs for robots.txt compliance
        test_urls = [
            "https://www.formula1.com/en/latest/",
            "https://www.motorsport.com/f1/",
            "https://www.fia.com/news",
        ]
        
        for url in test_urls:
            try:
                allowed = scraper.check_robots_txt(url)
                status = "âœ… ALLOWED" if allowed else "âŒ BLOCKED"
                print(f"  {status}: {url}")
            except Exception as e:
                print(f"  âš ï¸ ERROR checking {url}: {e}")
        
        print("\nğŸ›¡ï¸ Robots.txt compliance checking working!")
        
    except Exception as e:
        print(f"âŒ Error testing robots.txt: {e}")

def main():
    """Run all ethical scraping tests"""
    print("ğŸï¸ F1 Ethical Web Scraping Test Suite")
    print("=" * 60)
    
    # Test ethical scraping
    test_ethical_scraping()
    
    # Test robots.txt compliance
    test_robots_txt_compliance()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ All ethical scraping tests completed!")
    print("\nğŸ“– Key Takeaways:")
    print("  â€¢ Only RSS feeds and APIs are used by default")
    print("  â€¢ All scraping respects robots.txt")
    print("  â€¢ Rate limiting is implemented")
    print("  â€¢ Educational use only")
    print("  â€¢ Full ethical compliance")
    print("\nğŸ“š For more information, see: ETHICAL_SCRAPING_GUIDE.md")

if __name__ == "__main__":
    main()
